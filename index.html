<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ReSpAct</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <!-- <link rel="stylesheet" href="static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ReSpAct: Harmonizing Reasoning, Speaking, and Acting </h1>
            <h2 class="subtitle is-3 publication-subtitle">Towards Building Large Language Model-Based Conversational AI Agents</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://vardhandongre.github.io/" target="_blank">Vardhan Dongre</a></sup>,</span>
                <span class="author-block">
                  <a href="" target="_blank">Xiaocheng Yang</a></sup>,</span>
                  <span class="author-block">
                    <a href="https://emrecanacikgoz.github.io/" target="_blank">Emre Can Acikgoz</a></sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="" target="_blank">Suvodip Dey</a></sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://siebelschool.illinois.edu/about/people/faculty/gokhan" target="_blank">Gokhan Tur</a></sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://siebelschool.illinois.edu/about/people/faculty/dilek" target="_blank">Dilek Hakkani-Tür</a>
                  </span>
                  </div>

                  <img alt="Conversational AI Lab" src="static/images/ConvAIText.png" style="width:10%" />

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><a href="https://uiuc-conversational-ai-lab.github.io/" target="_blank">ConvAI Lab</a><br></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div> 
                  
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Illinois at Urbana Champaign<br></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2411.00927" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/vardhandongre/Respact" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Blog link -->
                <!-- <span class="link-block">
                  <a href="https://vardhandongre.notion.site/ReSpAct-Towards-Building-Large-Language-Model-Based-Conversational-AI-Agents-1342e61c204c8027a631e7427e570e61?pvs=4" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-book"></i>
                  </span>
                  <span>Blog</span>
                </a>
              </span> -->
              
              <!-- Notebook LM link -->
              <span class="link-block">
                <a href="https://notebooklm.google.com/notebook/5caa79d2-1bfb-46e0-8dbb-fbb997a5761a/audio" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-headphones"></i>
                </span>
                <span>Notebook LM</span>
              </a>
            </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span> -->
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Audio Player Section -->
<!-- <section class="section is-small">
  <div class="container has-text-centered">
      <h2 class="title is-4">Listen to Our Project Overview</h2>
      <audio controls class="custom-audio-player">
          <source src="static/audio/respact.wav" type="audio/wav">
          Your browser does not support the audio element.
      </audio>
  </div>
</section> -->

<!-- Audio Player Section -->
<!-- <section style="text-align: center; padding: 20px; background-color: #f9f9f9; border-radius: 8px; max-width: 400px; margin: 0 auto;">
  <h2 style="color: #333; margin-bottom: 1rem;">Listen to Our Project Overview</h2>
  <audio controls style="width: 80%; max-width: 300px; margin: 10px auto;">
    <source src="static/audio/respact.wav" type="audio/wav">
      Your browser does not support the audio element.
  </audio>
</section> -->


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language model (LLM)-based agents have been increasingly used to interact with external environments (e.g., games, APIs, etc.) and solve tasks. However, current frameworks do not enable these agents to work with users and interact with them to align on the details of their tasks and reach user-defined goals. Instead, in ambiguous situations, these agents may make decisions based on assumptions. This work introduces ReSpAct (Reason, Speak, and Act), a novel framework that synergistically combines essential skills for building task-oriented "conversational" agents. ReSpAct addresses this need for agents, expanding on the ReAct approach. The ReSpAct framework enables agents to interpret user instructions, reason about complex tasks, execute appropriate actions, and engage in dynamic dialogue to seek guidance, clarify ambiguities, understand user preferences, resolve problems, and use intermediate feedback and responses from users to update their plans.



          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End paper abstract -->


<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conversational Agents: Why They Matter?</h2>
        <div class="content has-text-justified">
          <p>
            In an increasingly complex digital landscape, conversational agents offer a way to bridge the gap between human needs and machine capabilities. These agents can assist us in navigating digital interfaces, support decision-making, and help accomplish goals that require contextual understanding.
          </p>
          <figure class="image is-inline-block" style="width: 70%;"> 
            <img src="static/images/framework2.png" alt="MY ALT TEXT"/>
          <p>
            <b>Existing approaches operate in a one-sided way</b>—executing commands without checking back for clarification. This approach can lead to errors and unmet expectations, especially in cases where precise details matter. However, with conversational agents, there is an added layer of communication, creating a back-and-forth that mirrors human conversation and allows users to guide the agent toward optimal outcomes.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Conversational Agents: Why They Matter?</h2>
        <div class="content has-text-justified">
          <p>
            In an increasingly complex digital landscape, conversational agents offer a way to bridge the gap between human needs and machine capabilities. These agents can assist us in navigating digital interfaces, support decision-making, and help accomplish goals that require contextual understanding.
          </p>
          <figure class="image is-inline-block"> 
            <img src="static/images/framework2.png" alt="MY ALT TEXT"/>
          </figure>
          <p>
            <b>Existing approaches operate in a one-sided way</b>—executing commands without checking back for clarification. This approach can lead to errors and unmet expectations, especially in cases where precise details matter. However, with conversational agents, there is an added layer of communication, creating a back-and-forth that mirrors human conversation and allows users to guide the agent toward optimal outcomes.
          </p>
        </div>     
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Why ReSpAct?</h2>
        <div class="content has-text-justified">
          <p>
            ReSpAct, or "Reason, Speak, Act," is a novel framework that builds on the ReAct approach to make interactions more human-centered. ReSpAct takes agent systems a step further by embedding conversations directly into the decision-making process, enabling agents to act autonomously while remaining responsive to user needs. For example, if an agent faces ambiguity, it can proactively ask, "Should I look for a blue sweater in the mens section, or do you have another preference?". It combines reasoning with conversational prompts to allow agents to clarify uncertainties, seek user feedback, and adapt strategies dynamically. This human-in-the-loop approach not only reduces mistakes but allows building user trust and satisfaction by keeping them engaged in real-time.
          </p>
          <figure class="image is-inline-block"> 
            <img src="static/images/examples.png" alt="MY ALT TEXT"/>
          </figure>
          <p>
            The ReSpAct framework empowers LLM-based Agents to execute tasks more accurately by blending autonomous reasoning with natural language interaction. When an agent is unclear on a user request, it can ask a question or share its reasoning, preventing it from acting based on assumptions. This dynamic engagement means fewer misunderstandings and more effective completion of tasks, as seen in tasks like booking accommodations or navigating e-commerce platforms.
          </p>
        </div>     
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div id="results-carousel" class="carousel results-carousel">
            
            <div class="item">
              <img src="static/images/framework.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                ReSpAct framework enables agents to interpret user instructions, reason about complex tasks, execute appropriate actions, and engage in dynamic dialogue.
              </h2>
            </div>

            <div class="item">
              <img src="static/images/efficient_respact_v2.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                Comparison of (a) ReAct and (b) ReSpAct to solve a game in AlfWorld
              </h2>
            </div>

            <div class="item">
              <img src="static/images/multiwoz.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                Comparison of (a) ReAct and (b) ReSpAct to set up a travel booking in MultiWOZ 
              </h2>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End image carousel -->

 <!-- Takeaway line -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">ReSpAct synergistically combines the essential skills for building task-oriented "conversational" agents. Agent can reasong through thoughts, interact with the user, and take actions in the environment to solve tasks.</h2>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">ReSpAct Approach</h2>
        <p>          
          ReSpAct expands the typical action space of LLMs by adding "speak" actions, allowing agents to ask for guidance, confirm details, and receive feedback during tasks. This means that instead of assuming and potentially making errors, the agent can ask, “Is this what you had in mind?” before proceeding. This interactive loop lets agents adapt on the fly, making ReSpAct ideal for complex tasks in interactive environments.
        </p>        
        <div class="has-text-centered"> 
          <figure class="image is-inline-block" style="width: 50%;"> 
            <img src="static/images/respact-ver3.png" alt="MY ALT TEXT"/>
          </figure>
          <p>
            Task-oriented conversational agents that ask questions, request feedback, and adapt their strategies based on user input.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Performance</h2>
        <div class="content has-text-justified">
          <p>
            We test ReSpAct in 3 settings: AlfWorld (Embodied Decision-Making), WebShop (Web Agent Decision-Making) and MultiWOZ (Task Oriented Dialogue System). Each of these environments presents unique challenges, requiring the agent to balance reasoning, dialogue, and action in different ways. To evaluate ReSpAct effectively, we design human-annotated trajectories with sparse occurences of reasoning traces and dialogs, allowing the language model to autonomously decide when to "think," "speak," or "act." This design simulates a real-world scenario where agents must not only follow instructions but also navigate uncertainties and dynamically adapt to the users needs.
          </p>
        </div>   
<!-- <section class="hero teaser">
  <div class="container is-max-desktop"> -->
    <h2 class="title is-3"> 
      AlfWorld 
    </h2>
    <p>
      We evaluate ReSpAct in AlfWorld, a synthetic environment based on the TextWorld framework and aligned with the embodied ALFRED benchmark. AlfWorld presents six task categories, such as finding hidden objects, moving items, and using objects together (e.g., heating a potato in a microwave). In this setting, ReSpAct enables agents to ask contextually relevant questions, provide status updates, and seek clarification to navigate tasks more effectively. Compared to a baseline ReAct agent, which relies on internal reasoning without user feedback, ReSpAct demonstrates notable improvements. It achieves an 87.3% success rate across tasks, outperforming ReAct's 80.6% by effectively integrating dialogue into its decision-making process.
    </p>
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/alfworld-respact.mp4"
        type="video/mp4">
      </video>
      <img src="static/images/table-alfworld.png" style="width: 60%;" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Comparison of (a) ReAct and (b) ReSpAct over Task-specific success rates (%) in Alfworld
      </h2>
      <figure class="image is-inline-block"> 
        <img src="static/images/sankey.png" style="width: 60%;" alt="MY ALT TEXT"/>
      </figure>
      <p>
        In our analysis we observed how ReSpAct changes the way agents handle tasks. While ReAct agents tend to jump straight into actions, ReSpAct introduces more "thinking" and "speaking" moments. The ReSpAct agents show a sharp reduction in "invalid actions"—down to just 3% from ReAct's 13%. Invalid actions are wasted steps, like trying to open an already open door, that dont contribute to task success.
      </p>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3">
      MultiWOZ
      </h2>
      <p>
        We evaluate ReSpAct in a MultiWOZ environment, where the agent assists with booking hotels, restaurants, and taxis by understanding user preferences, asking clarifying questions, and making informed decisions. Compared to a ReAct baseline that operates without user feedback, ReSpAct achieves higher scores on two metrics: Inform Rate, which checks if all requested information (e.g., hotel address and price) was correctly provided, and Success Rate, which further requires that all user goals, like completing a booking, are fully met. Using GPT-4o-mini, ReSpAct scores 72.2% on Inform and 51.8% on Success, compared to the baseline 66.7% and 48.8%, respectively. 
      </p>
      <br>
    <div class="hero-body" style="display: flex; flex-direction: column; align-items: center; justify-content: center;">
      <video poster="" id="tree" autoplay controls muted loop style="max-width: 100%; height: auto;">
        <!-- Your video here -->
        <source src="static/videos/respact-demo-woz-2.mov" type="video/mp4">
      </video>
      <img src="static/images/table-woz.png" style="width: 60%;" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Comparison of Inform and Success scores for MultiWOZ using GPT-4o-mini and Llama-405B-instruct models.
      </h2> 
        <img src="static/images/react_respact_dist_final.png" style="width: 50%;" alt="MY ALT TEXT"/>
      <h2>
        Comparison of Dialogue Distribution between ReSpAct and ReAct agents in MultiWOZ
      </h2>
    </div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3">
      WebShop
    </h2>
    <p>
      We evaluate ReSpAct in a WebShop environment, where the agent navigates a simulated online shopping platform containing over 1.18 million real-world products to identify and purchase items based on user preferences. The agent actively searches for products matching user requirements, asks clarifying questions to refine search results, and makes informed purchasing decisions. To assess performance, we use two metrics: (1) average score, which reflects the percentage of desired attributes met by the chosen product across all episodes, and (2) success rate, which measures the percentage of episodes where the chosen product fully satisfies the users requirements. Our results show that ReSpAct achieves an average score of 32.7% and a success rate of 12% with a user simulator. With human interaction, ReSpAct reaches an 85.8% average score and 50% success rate, underscoring the critical role of interactive questioning and feedback in enhancing accuracy and user satisfaction with task-oriented web shopping agents.
    </p>
    <br>
    <div class="hero-body" style="display: flex; flex-direction: column; align-items: center; justify-content: center;">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/respact-demo-webshop.mov"
        type="video/mp4">
      </video>
      <img src="static/images/table-shop.png" alt="MY ALT TEXT" style="width: 50%;"/>
      <h2 class="subtitle has-text-centered">
        Comparison of Score and success rate (SR) on 100 Test WebShop trajectories using GPT-4o-mini
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<section class="section" id="Analysis">
    <div class="container is-max-desktop content">
      <h2 class="title">
       Conversational behavior Distribution
      </h2>
      <div class="hero-body" style="display: flex; flex-direction: column; align-items: center; justify-content: center;">
      
      <!-- to adjust image size use style="width: 50%;" -->
        <img src="static/images/response_dist_final.png" style="width: 50%;" alt="MY ALT TEXT"/>
      <p>
        A detailed breakdown of different dialogues in all 3 settings as shown in Figure reveals distinct conversational patterns of the ReSpAct agent across domains.
      </p>
      </div>
  </div>
</section>

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->

<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Implications and the Future of Conversational Agents</h2>
    <p>
      By embedding conversational checkpoints within its reasoning process, ReSpAct addresses longstanding issues like error propagation, task ambiguity, and rigid agent behavior. In interactive environments like WebShop and AlfWorld, ReSpAct's approach not only reduces errors during task completion but also enables agents to align more closely with user intent by actively involving users in decision-making. This responsiveness transforms the agent into a collaborative partner rather than a passive tool.
    </p>
  </section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{dongre2024respactharmonizingreasoningspeaking,
        title={ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents}, 
        author={Vardhan Dongre and Xiaocheng Yang and Emre Can Acikgoz and Suvodip Dey and Gokhan Tur and Dilek Hakkani-Tür},
        year={2024},
        eprint={2411.00927},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/2411.00927}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!-- Acknowledgements -->
<section class="section" id="Acknowledgements">
<div class="container is-max-desktop content">
  <h2 class="title">Acknowledgements</h2>
  <p>
    This work was supported in part by Other Transaction award HR0011249XXX from the U.S. Defense Advanced Research Projects Agency (DARPA) Friction for Accountability in Conversational Transactions (FACT) program and has benefited from the Microsoft Accelerate Foundation Models Research (AFMR) grant program, through which leading foundation models hosted by Microsoft Azure and access to Azure credits were provided to conduct the research.
  </p>
</section>

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website's template is borrowed from <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. We thank the authors for open-sourcing their code.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
